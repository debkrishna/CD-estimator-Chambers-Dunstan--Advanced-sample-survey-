{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Updating_the_prior.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/debkrishna/CD-estimator-Chambers-Dunstan--Advanced-sample-survey-/blob/main/Updating_the_prior.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69qm6tpKKZRp",
        "outputId": "58d881cd-9f2d-4e79-b125-2bbfd381a05d"
      },
      "source": [
        "import os\n",
        "import psycopg2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from psycopg2.extras import DictCursor\n",
        "import pymc3 as pm\n",
        "from pymc3.distributions import Interpolated\n",
        "import theano.tensor as tt\n",
        "from scipy import stats\n",
        "from model import create_submodel_1, create_submodel_2\n",
        "from training import train_submodel_1,train_submodel_2,get_situation,get_outcome,initialise"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzvNNq2BKZRs"
      },
      "source": [
        "def get_queryset(league = 'ICC T20'):\n",
        "    \"\"\"\n",
        "    get queryset for model from database.\n",
        "    match_id - id of the match\n",
        "    inning - match inning\n",
        "    team1_id - batting team 1 id\n",
        "    team2_id - bowling team 2 id\n",
        "    over - over of the ball\n",
        "    ball - ball of the inning\n",
        "    batsmen_id - id of batsmen\n",
        "    non_striker_id - id of non striker batsmen\n",
        "    bowler_id - id of the bowler\n",
        "    is_wide - True/False\n",
        "    is_no_ball - True/False\n",
        "    total_runs - total run till now\n",
        "    is_out - True/False\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    connection = psycopg2.connect(\n",
        "        database=\"cricket\",\n",
        "        user=\"postgres\",\n",
        "        password=\"ynKrco0QWa56iez8sKtG\",\n",
        "        host=\"trusted-prediction-research.c8qtxwbvasgw.ap-south-1.rds.amazonaws.com\",\n",
        "        port=\"5432\"\n",
        "    )\n",
        "    cursor = connection.cursor(cursor_factory=DictCursor)\n",
        "    cursor.execute(\n",
        "        \"\"\"\n",
        "        select\n",
        "            match_id, inning, css.team_id, opp_team_id, com.over,\n",
        "            ball, batsmen_id, offstrike_batsmen_id non_striker_id,\n",
        "            bowler_id, is_wide, is_no_ball, is_out, is_caugth,\n",
        "            player_out_id, com.runs, total_runs\n",
        "        from cds_commentary_commentary com\n",
        "        inner join cds_scorecard_scorecard css on com.scorecard_id = css.id\n",
        "        inner join cds_match_match cmm on css.match_id = cmm.id\n",
        "        inner join cds_series_series c on cmm.series_id = c.id\n",
        "        inner join cds_league_league cll on c.league_id = cll.id\n",
        "        where cll.name = 'ICC T20'\n",
        "        limit 1000\n",
        "        \"\"\".format(league=league)\n",
        "    )\n",
        "    queryset = cursor.fetchall()\n",
        "    connection.close()\n",
        "    return queryset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9YoR5KrKZRt"
      },
      "source": [
        "def load_data():\n",
        "    '''\n",
        "    Load data from queryset\n",
        "    '''\n",
        "    queryset = get_queryset()\n",
        "    result = []\n",
        "    for row in queryset:\n",
        "        result.append(dict(row))\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8z83uY3KZRt"
      },
      "source": [
        "### All previous matches Data upto the current date.\n",
        "#### But we should consider only recent matches to capture the updated likelihood. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuYxpfZ6KZRu"
      },
      "source": [
        "def train_submodel_1_new(n_iter, target_accept, I, J, id1, id2, X, save_directory):\n",
        "    '''\n",
        "    Initialises and trains submodel 1.\n",
        "    Creates save directory if it doesn't exist.\n",
        "    '''\n",
        "    wickets_sub_model = create_submodel_1(I, J, id1, id2, X)\n",
        "\n",
        "    with wickets_sub_model:\n",
        "        trace = pm.sample(n_iter, target_accept=target_accept, cores=1)\n",
        "\n",
        "    cutpoints1 = np.mean(trace.get_values(\"cutpoints1\", burn=n_iter // 2, combine=True), axis=0)\n",
        "    mu1 = np.mean(trace.get_values(\"mu1\", burn=n_iter // 2, combine=True), axis=0)\n",
        "    mu2 = np.mean(trace.get_values(\"mu2\", burn=n_iter // 2, combine=True), axis=0)\n",
        "    delta11 = np.mean(trace.get_values(\"delta11\", burn=n_iter // 2, combine=True), axis=0)\n",
        "    delta12 = np.mean(trace.get_values(\"delta12\", burn=n_iter // 2, combine=True), axis=0)\n",
        "    delta13 = np.mean(trace.get_values(\"delta13\", burn=n_iter // 2, combine=True), axis=0)\n",
        "    delta1 = [delta11, delta12, delta13, delta11, delta12, delta13, delta11, delta12, delta13]\n",
        "\n",
        "    cutpoints1_values = trace.get_values(\"cutpoints1\", burn=n_iter // 2, combine=True)\n",
        "    mu1_values = trace.get_values(\"mu1\", burn=n_iter // 2, combine=True)\n",
        "    mu2_values = trace.get_values(\"mu2\", burn=n_iter // 2, combine=True)\n",
        "    delta11_values = trace.get_values(\"delta11\", burn=n_iter // 2, combine=True)\n",
        "    delta12_values = trace.get_values(\"delta12\", burn=n_iter // 2, combine=True)\n",
        "    delta13_values = trace.get_values(\"delta13\", burn=n_iter // 2, combine=True)\n",
        "    delta1_values = [delta11_values, delta12_values, delta13_values, delta11_values, delta12_values, delta13_values, delta11_values, delta12_values, delta13_values]\n",
        "    \n",
        "\n",
        "    return delta1, mu1, mu2, cutpoints1,delta11_values, delta12_values, delta13_values,mu1_values, mu2_values, cutpoints1_values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-0Gjf_CKZRv",
        "outputId": "3f3c17bb-42d5-4523-ce85-d600f04748c7"
      },
      "source": [
        "target_accept = 0.95  # doesn't need to be changed\n",
        "n_iter = 500\n",
        "save_directory = \"save\"\n",
        "deliveries_data = load_data()\n",
        "\n",
        "batsmen, bowlers, batsman_index, bowler_index, X, id1, id2, noballs_and_wides = initialise(deliveries_data[:954])\n",
        "\n",
        "A = train_submodel_1_new(n_iter, target_accept, len(batsmen), len(bowlers), id1, id2, X,\n",
        "                 save_directory)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Auto-assigning NUTS sampler...\n",
            "Initializing NUTS using jitter+adapt_diag...\n",
            "Sequential sampling (2 chains in 1 job)\n",
            "NUTS: [cutpoints1, mu2, mu1, tau2, tau1, delta13, delta12, delta11]\n",
            "100%|██████████| 1000/1000 [00:53<00:00, 18.60it/s]\n",
            "100%|██████████| 1000/1000 [00:50<00:00, 19.82it/s]\n",
            "The estimated number of effective samples is smaller than 200 for some parameters.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJY6bJhLKZRv"
      },
      "source": [
        "Hist_data = initialise(deliveries_data[:954])\n",
        "part_data = deliveries_data[953:]\n",
        "new_match_data_ini  = initialise(part_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fHyFjoiDLwf"
      },
      "source": [
        "batsman_present = []\r\n",
        "for i in range(len(new_match_data_ini[0])):\r\n",
        "  if (new_match_data_ini[0][i] in Hist_data[0]) == True:\r\n",
        "    batsman_present.append(new_match_data_ini[0][i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8y-MW2NMKfW"
      },
      "source": [
        "bowler_present = []\r\n",
        "for i in range(len(new_match_data_ini[1])):\r\n",
        "  if (new_match_data_ini[1][i] in Hist_data[1]) == True:\r\n",
        "    bowler_present.append(new_match_data_ini[1][i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QeBcx_rIxgD"
      },
      "source": [
        "p_dt = pd.DataFrame(part_data)\r\n",
        "n_m_d = p_dt[(p_dt.batsmen_id.isin(batsman_present)) & (p_dt.bowler_id.isin(bowler_present))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4PHdZcuR9Fu"
      },
      "source": [
        "n_m_d_l = list(n_m_d.to_dict(orient='records'))\r\n",
        "new_match_data  = initialise(n_m_d_l)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZqAAzK2KZRv"
      },
      "source": [
        "def transformed_index(Hist_data,new_match_data):\n",
        "    trans_batsman_index = {}\n",
        "    for i in new_match_data[0]:\n",
        "        batsman_index = Hist_data[2]\n",
        "        trans_batsman_index[i] = batsman_index.get(i)\n",
        "    trans_bowler_index = {}\n",
        "    for i in new_match_data[1]:\n",
        "        bowler_index = Hist_data[3]\n",
        "        trans_bowler_index[i] = bowler_index.get(i)\n",
        "    for i in range(len(new_match_data[5])):\n",
        "        for j in range(len((new_match_data[5])[i])):\n",
        "            new_value = (new_match_data[5])[i][j]\n",
        "            k = list(new_match_data[2].keys())[list(new_match_data[2].values()).index(new_value)]\n",
        "            (new_match_data[5])[i][j] = trans_batsman_index[k]  \n",
        "    for i in range(len(new_match_data[6])):\n",
        "        for j in range(len((new_match_data[6])[i])):\n",
        "            new_value = (new_match_data[6])[i][j]\n",
        "            k = list(new_match_data[3].keys())[list(new_match_data[3].values()).index(new_value)]\n",
        "            (new_match_data[6])[i][j] = trans_bowler_index[k]\n",
        "            # id1 and id2 of new_match_data will be transformed\n",
        "    return(trans_batsman_index,trans_bowler_index,new_match_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CgUnyRZKZRw"
      },
      "source": [
        "\n",
        "def from_posterior(param, samples):\n",
        "    smin, smax = np.min(samples), np.max(samples)\n",
        "    width = smax - smin\n",
        "    x = np.linspace(smin, smax, 100)\n",
        "    y = stats.gaussian_kde(samples)(x)#we can use gridsearch for best fitting kernal\n",
        "\n",
        "    x = np.concatenate([[x[0] - 3 * width], x, [x[-1] + 3 * width]])\n",
        "    y = np.concatenate([[0], y, [0]])\n",
        "    return Interpolated(param, x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN06sVdwKZRx"
      },
      "source": [
        "trans_batsmen = list(transformed_index(Hist_data,new_match_data)[0].values())\n",
        "trans_bowlers = list(transformed_index(Hist_data,new_match_data)[1].values())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0KDglr8KZRx"
      },
      "source": [
        "def updated_submodel_1(trans_batsmen,trans_bowlers, id1, id2, X):\n",
        "    '''\n",
        "    Submodel 1: models the fall of wickets.\n",
        "    '''\n",
        "    X1 = [[1 if [l][i] == 1 else 2 for i in range(len(X[l]))] for l in range(9)]\n",
        "    X1 = np.asarray([np.array(X1[i]) for i in range(9)], dtype=object)\n",
        "    new_model = pm.Model()\n",
        "\n",
        "    with new_model:\n",
        "        delta11 = (from_posterior(\"delta11\",A[4] ))\n",
        "        delta12 = (from_posterior(\"delta12\", A[5]))\n",
        "        delta13 = (from_posterior(\"delta13\", A[6]))\n",
        "        delta1 = [delta11, delta12, delta13, delta11, delta12, delta13, delta11, delta12, delta13]\n",
        "        mu1 = ([from_posterior(\"mu1\"+ str(i),A[7][:,i]) for i in trans_batsmen])\n",
        "        mu2 = ([from_posterior(\"mu2\"+ str(i),A[8][:,i]) for i in trans_bowlers])\n",
        "        mu1 = tt.as_tensor_variable(mu1)\n",
        "        mu2 = tt.as_tensor_variable(mu2)\n",
        "        eta1 = [pm.Deterministic(\"eta1_\" + str(i), delta1[i] + mu1[id1[i]] - mu2[id2[i]]) for i in range(9)]\n",
        "\n",
        "        cutpoints1 = from_posterior(\"cutpoints1\",A[9].reshape(-1))\n",
        "\n",
        "        X1_ = [pm.OrderedLogistic(\"X1_\" + str(i), cutpoints=cutpoints1, eta=eta1[i], observed=X1[i] - 1) for i in\n",
        "               range(9)]\n",
        "        step = pm.Metropolis()\n",
        "        updated_trace = pm.sample(step = step)\n",
        "        mu1_est_values = [updated_trace[\"mu1\"+str(i)] for i in trans_batsmen]\n",
        "        mu2_est_values = [updated_trace[\"mu2\"+str(i)] for i in trans_bowlers]\n",
        "        mu1_est = np.array(mu1_est_values).mean(axis = 1)\n",
        "        mu2_est = np.array(mu2_est_values).mean(axis = 1)\n",
        "        delta11_est = updated_trace[\"delta11\"].mean()\n",
        "        delta12_est = updated_trace[\"delta12\"].mean()\n",
        "        delta13_est = updated_trace[\"delta13\"].mean()\n",
        "        cutpoints1_est= updated_trace[\"cutpoints1\"].mean()\n",
        "        delta1_est = [delta11_est, delta12_est, delta13_est, delta11_est, delta12_est, delta13_est, delta11_est, delta12_est, delta13_est]\n",
        "    return delta1_est, mu1_est, mu2_est, cutpoints1_est\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwZZj120KZRy",
        "outputId": "e688dd3e-b5df-49e0-8c30-3c30c62c3a65"
      },
      "source": [
        "X = new_match_data[4]\n",
        "S = updated_submodel_1(trans_batsmen,trans_bowlers, new_match_data[5], new_match_data[6],X)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential sampling (2 chains in 1 job)\n",
            "CompoundStep\n",
            ">Metropolis: [cutpoints1]\n",
            ">Metropolis: [mu213]\n",
            ">Metropolis: [mu123]\n",
            ">Metropolis: [mu141]\n",
            ">Metropolis: [delta13]\n",
            ">Metropolis: [delta12]\n",
            ">Metropolis: [delta11]\n",
            "100%|██████████| 1000/1000 [00:03<00:00, 271.77it/s]\n",
            "100%|██████████| 1000/1000 [00:02<00:00, 403.21it/s]\n",
            "The estimated number of effective samples is smaller than 200 for some parameters.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8njJc2VrKZRy"
      },
      "source": [
        "def train_submodel_2_new(n_iter, target_accept, I, J, id1, id2, X, save_directory):\n",
        "    '''\n",
        "    Initialises and trains submodel 2.\n",
        "    Creates save directory if it doesn't exist.\n",
        "    '''\n",
        "    wickets_sub_model = create_submodel_2(I, J, id1, id2, X)\n",
        "\n",
        "    with wickets_sub_model:\n",
        "        trace = pm.sample(n_iter, target_accept=target_accept, cores=1)\n",
        "\n",
        "    cutpoints2 = np.mean(trace.get_values(\"cutpoints2\", burn=n_iter // 2, combine=True), axis=0)\n",
        "    mu3 = np.mean(trace.get_values(\"mu3\", burn=n_iter // 2, combine=True), axis=0)\n",
        "    mu4 = np.mean(trace.get_values(\"mu4\", burn=n_iter // 2, combine=True), axis=0)\n",
        "    delta21 = np.mean(trace.get_values(\"delta21\", burn=n_iter // 2, combine=True), axis=0)\n",
        "    delta22 = np.mean(trace.get_values(\"delta22\", burn=n_iter // 2, combine=True), axis=0)\n",
        "    delta2 = np.greater_equal([i for i in range(9)], 3) * delta21 + np.greater_equal([i for i in range(9)], 6) * delta22\n",
        "    cutpoints2_values = trace.get_values(\"cutpoints2\", burn=n_iter // 2, combine=True)\n",
        "    mu3_values = trace.get_values(\"mu3\", burn=n_iter // 2, combine=True)\n",
        "    mu4_values = trace.get_values(\"mu4\", burn=n_iter // 2, combine=True)\n",
        "    delta21_values = trace.get_values(\"delta21\", burn=n_iter // 2, combine=True)\n",
        "    delta22_values = trace.get_values(\"delta22\", burn=n_iter // 2, combine=True)\n",
        "\n",
        "    return delta2, mu3, mu4, cutpoints2,delta21_values, delta22_values,mu3_values, mu4_values, cutpoints2_values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YP5cko5KZRz",
        "scrolled": true,
        "outputId": "633e0a27-a8dd-4030-bc08-75d768ae51b6"
      },
      "source": [
        "target_accept = 0.95  # doesn't need to be changed\n",
        "n_iter = 500\n",
        "save_directory = \"save\"\n",
        "deliveries_data = load_data()\n",
        "\n",
        "batsmen, bowlers, batsman_index, bowler_index, X, id1, id2, noballs_and_wides = initialise(deliveries_data)\n",
        "\n",
        "B = train_submodel_2_new(n_iter, target_accept, len(batsmen), len(bowlers), id1, id2, X,\n",
        "                 save_directory)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Auto-assigning NUTS sampler...\n",
            "Initializing NUTS using jitter+adapt_diag...\n",
            "Sequential sampling (2 chains in 1 job)\n",
            "NUTS: [cutpoints2, mu4, mu3, tau2, tau1, delta22, delta21]\n",
            "100%|██████████| 1000/1000 [00:56<00:00, 17.81it/s]\n",
            "100%|██████████| 1000/1000 [00:36<00:00, 27.27it/s]\n",
            "The number of effective samples is smaller than 25% for some parameters.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kzql6kDnKZR0"
      },
      "source": [
        "def myfilter(X, A):\n",
        "    '''\n",
        "    Helper function to filter parameters in submodel 2\n",
        "    '''\n",
        "    ret = []\n",
        "    for l in range(9):\n",
        "        tmp = []\n",
        "        for i in range(len(X[l])):\n",
        "            if X[l][i] != 1:\n",
        "                tmp.append(A[l][i])\n",
        "        ret.append(tmp)\n",
        "    ret = np.asarray([np.array(ret[i]) for i in range(9)], dtype=object)\n",
        "    return ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_D_M8PIKZR0"
      },
      "source": [
        "def updated_submodel_2(trans_batsmen,trans_bowlers, id1, id2, X):\n",
        "    '''\n",
        "    Submodel 2: models runs scored.\n",
        "    '''\n",
        "    X1 = [[1 if X[l][i] == 1 else 2 for i in range(len(X[l]))] for l in range(9)]\n",
        "    X1 = np.asarray([np.array(X1[i]) for i in range(9)], dtype=object)\n",
        "    id3 = myfilter(X, id1)\n",
        "    id4 = myfilter(X, id2)\n",
        "    X2 = myfilter(X, X) - 1\n",
        "    l = [i for i in range(9)]\n",
        "    new_model = pm.Model()\n",
        "    with new_model:\n",
        "        delta21 = (from_posterior(\"delta21\",B[4] ))\n",
        "        delta22 = (from_posterior(\"delta22\", B[5]))\n",
        "        delta2 = pm.math.ge(l, 3) * delta21 + pm.math.ge(l, 6) * delta22  # offset for different situations\n",
        "        mu3 = ([from_posterior(\"mu3\"+ str(i),B[6][:,i]) for i in trans_batsmen])\n",
        "        mu4 = ([from_posterior(\"mu4\"+ str(i),B[7][:,i]) for i in trans_bowlers])\n",
        "        mu3 = tt.as_tensor_variable(mu3)\n",
        "        mu4 = tt.as_tensor_variable(mu4)\n",
        "        eta2 = [pm.Deterministic(\"eta2_\" + str(i), delta2[i] + mu3[id3[i]] - mu4[id4[i]]) for i in range(9)]\n",
        "        \n",
        "        cutpoints2 = [[from_posterior(\"cutpoints2\"+str(j)+str(i),B[8][:,j,i]) for i in range(5)] for j in range(9)]\n",
        "\n",
        "        X2_ = [pm.OrderedLogistic(\"X2_\" + str(i), cutpoints=cutpoints2[i], eta=eta2[i], observed=X2[i] - 1) for i in\n",
        "               range(9)]\n",
        "        step = pm.Metropolis()\n",
        "        updated_trace = pm.sample(step = step)\n",
        "        mu3_est_values = [updated_trace[\"mu3\"+str(i)] for i in trans_batsmen]\n",
        "        mu4_est_values = [updated_trace[\"mu4\"+str(i)] for i in trans_bowlers]\n",
        "        mu3_est = np.array(mu3_est_values).mean(axis = 1)\n",
        "        mu4_est = np.array(mu4_est_values).mean(axis = 1)\n",
        "        delta21_est_values = updated_trace[\"delta21\"]\n",
        "        delta22_est_values = updated_trace[\"delta22\"]\n",
        "        delta21_est = updated_trace[\"delta21\"].mean()\n",
        "        delta22_est = updated_trace[\"delta22\"].mean()\n",
        "        cutpoints2_est_values = [[updated_trace[\"cutpoints2\"+str(j)+str(i)] for i in range(5)] for j in range(9)]\n",
        "        cutpoints2_est = np.array(cutpoints2_est_values).mean(axis = 2)\n",
        "        delta2_est = np.greater_equal([i for i in range(9)], 3) * delta21_est + np.greater_equal([i for i in range(9)], 6) * delta22_est\n",
        "    return delta2_est, mu3_est, mu4_est,cutpoints2_est,delta21_est_values,delta22_est_values,mu3_est_values,mu4_est_values,cutpoints2_est_values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GWuDVICKZR0",
        "scrolled": false,
        "outputId": "11649edb-7eb8-4e54-c950-49314fdbd30f"
      },
      "source": [
        "X = new_match_data[4]\n",
        "R = updated_submodel_2(trans_batsmen,trans_bowlers, new_match_data[5], new_match_data[6],X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential sampling (2 chains in 1 job)\n",
            "CompoundStep\n",
            ">Metropolis: [cutpoints284]\n",
            ">Metropolis: [cutpoints283]\n",
            ">Metropolis: [cutpoints282]\n",
            ">Metropolis: [cutpoints281]\n",
            ">Metropolis: [cutpoints280]\n",
            ">Metropolis: [cutpoints274]\n",
            ">Metropolis: [cutpoints273]\n",
            ">Metropolis: [cutpoints272]\n",
            ">Metropolis: [cutpoints271]\n",
            ">Metropolis: [cutpoints270]\n",
            ">Metropolis: [cutpoints264]\n",
            ">Metropolis: [cutpoints263]\n",
            ">Metropolis: [cutpoints262]\n",
            ">Metropolis: [cutpoints261]\n",
            ">Metropolis: [cutpoints260]\n",
            ">Metropolis: [cutpoints254]\n",
            ">Metropolis: [cutpoints253]\n",
            ">Metropolis: [cutpoints252]\n",
            ">Metropolis: [cutpoints251]\n",
            ">Metropolis: [cutpoints250]\n",
            ">Metropolis: [cutpoints244]\n",
            ">Metropolis: [cutpoints243]\n",
            ">Metropolis: [cutpoints242]\n",
            ">Metropolis: [cutpoints241]\n",
            ">Metropolis: [cutpoints240]\n",
            ">Metropolis: [cutpoints234]\n",
            ">Metropolis: [cutpoints233]\n",
            ">Metropolis: [cutpoints232]\n",
            ">Metropolis: [cutpoints231]\n",
            ">Metropolis: [cutpoints230]\n",
            ">Metropolis: [cutpoints224]\n",
            ">Metropolis: [cutpoints223]\n",
            ">Metropolis: [cutpoints222]\n",
            ">Metropolis: [cutpoints221]\n",
            ">Metropolis: [cutpoints220]\n",
            ">Metropolis: [cutpoints214]\n",
            ">Metropolis: [cutpoints213]\n",
            ">Metropolis: [cutpoints212]\n",
            ">Metropolis: [cutpoints211]\n",
            ">Metropolis: [cutpoints210]\n",
            ">Metropolis: [cutpoints204]\n",
            ">Metropolis: [cutpoints203]\n",
            ">Metropolis: [cutpoints202]\n",
            ">Metropolis: [cutpoints201]\n",
            ">Metropolis: [cutpoints200]\n",
            ">Metropolis: [mu413]\n",
            ">Metropolis: [mu323]\n",
            ">Metropolis: [mu341]\n",
            ">Metropolis: [delta22]\n",
            ">Metropolis: [delta21]\n",
            "100%|██████████| 1000/1000 [01:06<00:00, 15.03it/s]\n",
            "100%|██████████| 1000/1000 [01:09<00:00, 14.35it/s]\n",
            "The gelman-rubin statistic is larger than 1.05 for some parameters. This indicates slight problems during sampling.\n",
            "The estimated number of effective samples is smaller than 200 for some parameters.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}